{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQr4uHqjA1jo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from networkx import *\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "from collections import Counter\n",
        "import time\n",
        "import pickle\n",
        "import os\n",
        "!pip install powerlaw\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "El_Y_1se5ZgP"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WqfmevB_fuMM"
      },
      "outputs": [],
      "source": [
        "file =\"/content/drive/MyDrive/NI_Research/NI_Paper-Author_with_attributes_exp5.csv\"\n",
        "paper_author = pd.read_csv(file, keep_default_na=False, encoding = 'utf-8',low_memory=False)\n",
        "#u = paper_author.select_dtypes(object)\n",
        "#paper_author[u.columns] = u.apply(\n",
        "#    lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GrHHBvKc_E-"
      },
      "outputs": [],
      "source": [
        "paper_author"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jr9Gb7legTLQ"
      },
      "outputs": [],
      "source": [
        "##check formats of columns\n",
        "paper_author = paper_author.convert_dtypes()\n",
        "paper_author[['Paper_Year','Author_NI_StartDate', 'Author_NI_EndDate', 'Author_YearJoinedUniversity', 'Author_YearDegree', 'Author_Department1', 'Paper_PubVenue_combined']].dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gkUa1mbn98T"
      },
      "outputs": [],
      "source": [
        "## convert year column to int\n",
        "paper_author['Author_YearDegree'] = pd.to_numeric(paper_author['Author_YearDegree'], errors = 'coerce')\n",
        "paper_author['Author_YearDegree'] = paper_author['Author_YearDegree'].fillna(0).astype(int)\n",
        "paper_author['Author_NI_StartDate'] = pd.to_numeric(paper_author['Author_NI_StartDate'], errors = 'coerce')\n",
        "paper_author['Author_NI_StartDate'] = paper_author['Author_NI_StartDate'].fillna(0).astype(int)\n",
        "paper_author['Author_NI_EndDate'] = pd.to_numeric(paper_author['Author_NI_EndDate'], errors = 'coerce')\n",
        "paper_author['Author_NI_EndDate'] = paper_author['Author_NI_EndDate'].fillna(0).astype(int)\n",
        "paper_author['Author_YearJoinedUniversity'] = pd.to_numeric(paper_author['Author_YearJoinedUniversity'], errors = 'coerce')\n",
        "paper_author['Author_YearJoinedUniversity'] = paper_author['Author_YearJoinedUniversity'].fillna(0).astype(int)\n",
        "paper_author['Author_NI'] = pd.to_numeric(paper_author['Author_NI'], errors = 'coerce')\n",
        "paper_author['Author_NI'] = paper_author['Author_NI'].fillna(0).astype(int)\n",
        "paper_author['Paper_Year'] = pd.to_numeric(paper_author['Paper_Year'], errors = 'coerce')\n",
        "paper_author['Paper_Year'] = paper_author['Paper_Year'].fillna(0).astype(int)\n",
        "paper_author['Author_subset_exp5'] = pd.to_numeric(paper_author['Author_subset_exp5'], errors = 'coerce')\n",
        "paper_author['Author_subset_exp5'] = paper_author['Author_subset_exp5'].fillna(0).astype(int)\n",
        "#paper_author['Author_Department1'] = paper_author['Author_Department1'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXVnixez22eM"
      },
      "outputs": [],
      "source": [
        "paper_author[['Author_subset_exp5','Paper_Year','Author_NI_StartDate', 'Author_NI_EndDate', 'Author_YearJoinedUniversity', 'Author_YearDegree', 'Author_Department1','Paper_PubVenue_combined']].dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytC7rQSUiHgG"
      },
      "outputs": [],
      "source": [
        "## must be imported as nx.DiGraph to preserve order of source,target (paper,author)\n",
        "paper_author_net = nx.from_pandas_edgelist(paper_author, source='Source', target='Target', edge_attr=True, create_using=nx.DiGraph())\n",
        "#undirected version\n",
        "#paper_author_net_undirected = nx.from_pandas_edgelist(paper_author, source='Source', target='Target', edge_attr=True, create_using=nx.OrderedGraph())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jr0qnBudj2WB"
      },
      "outputs": [],
      "source": [
        "# Add node attributes\n",
        "nodelist= pd.read_csv('/content/drive/MyDrive/NI_Research/NI_Author_Nodelist_exp5.csv')\n",
        "u2 = nodelist.select_dtypes(object)\n",
        "nodelist[u2.columns] = u2.apply(\n",
        "    lambda x: x.str.encode('ascii', 'ignore').str.decode('ascii'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d49h1PS9lkeV"
      },
      "outputs": [],
      "source": [
        "for i, nlrow in nodelist.iterrows():\n",
        "  try:\n",
        "    paper_author_net.nodes[nlrow['Target']].update(nlrow[1:].to_dict())\n",
        "  except:\n",
        "    continue\n",
        "print(nlrow)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJoMLGIyoVWW"
      },
      "outputs": [],
      "source": [
        "nodelist = nodelist.convert_dtypes()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hbhIvCmomNe"
      },
      "outputs": [],
      "source": [
        "nodelist['Author_YearJoinedUniversity'] = pd.to_numeric(nodelist['Author_YearJoinedUniversity'], errors = 'ignore')\n",
        "nodelist['Author_YearJoinedUniversity'] = nodelist['Author_YearJoinedUniversity'].fillna(0).astype(int)\n",
        "nodelist['Author_YearDegree'] = pd.to_numeric(nodelist['Author_YearDegree'], errors = 'ignore')\n",
        "nodelist['Author_YearDegree'] = nodelist['Author_YearDegree'].fillna(0).astype(int)\n",
        "nodelist['Author_NI_StartDate'] = pd.to_numeric(nodelist['Author_NI_StartDate'], errors = 'ignore')\n",
        "nodelist['Author_NI_StartDate'] = nodelist['Author_NI_StartDate'].fillna(0).astype(int)\n",
        "nodelist['Author_NI_EndDate'] = pd.to_numeric(nodelist['Author_NI_EndDate'], errors = 'ignore')\n",
        "nodelist['Author_NI_EndDate'] = nodelist['Author_NI_EndDate'].fillna(0).astype(int)\n",
        "nodelist['Author_NI'] = pd.to_numeric(nodelist['Author_NI'], errors = 'ignore')\n",
        "nodelist['Author_NI'] = nodelist['Author_NI'].fillna(0).astype(int)\n",
        "#nodelist['Author_Department1'] = nodelist['Author_Department1'].astype(str)\n",
        "nodelist['Author_subset_exp5'] = pd.to_numeric(nodelist['Author_subset_exp5'], errors = 'ignore')\n",
        "nodelist['Author_subset_exp5'] = nodelist['Author_subset_exp5'].fillna(0).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJXhpFT03jwD"
      },
      "outputs": [],
      "source": [
        "nodelist[['Author_YearJoinedUniversity','Author_subset_exp5','Author_NI_StartDate','Author_NI_EndDate', 'Author_YearDegree', 'Author_Department1']].dtypes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NI_only_nodelist = pd.read_csv('/content/drive/MyDrive/NI_Research/Exp5_ego_intersection.csv')"
      ],
      "metadata": {
        "id": "-LGoQvD4N1jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H44yfTxf330c"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyTNgTNnd4uz"
      },
      "source": [
        "**Experiment 5:** 4 years publications before & after + *0* year gap between before-after"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyXAhqx6d4u0"
      },
      "outputs": [],
      "source": [
        "# Step 1: subset the network to include nodes with appropriate YearDegree & YearJoin\n",
        "exp5_subset = [(u,v) for u,v,e in paper_author_net.edges(data=True) if e['Author_NI_StartDate'] <= 2021-4 and e['Author_YearDegree'] <= e['Author_NI_StartDate']-4 and e['Author_NI_StartDate'] != 0 and e['Author_YearDegree'] != 0 and e['Author_NI_StartDate'] != 9999 and e['Author_YearDegree'] != 9999 and e['Author_Department1'] != '9999' and e['Paper_PubVenue_combined'] != '0' and e['Author_subset_exp5'] == 1]\n",
        "exp5_subset_df = pd.DataFrame(exp5_subset, columns = ['Source','Target'])\n",
        "exp5_subset_attributes = exp5_subset_df.merge(paper_author,how='left',on=('Source','Target'))\n",
        "exp5_subset_net = nx.from_pandas_edgelist(exp5_subset_attributes, source='Source', target='Target',edge_attr=True, create_using=nx.DiGraph())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXY2nv_GIrnY"
      },
      "outputs": [],
      "source": [
        "#Step 2: preparation of BEFORE-NI: subset paper-author edges with appropriate pub range & gap size before-after\n",
        "exp5_before = [(u,v) for u,v,e in exp5_subset_net.edges(data=True) if e['Author_NI_StartDate'] - e['Paper_Year'] <= 4 and e['Author_NI_StartDate'] - e['Paper_Year'] >= 1 and e['Author_NI_StartDate'] != 0 and e['Paper_Year'] != 0 and e['Author_NI_StartDate'] != 9999 and e['Author_YearDegree'] != 9999]\n",
        "exp5_before_df = pd.DataFrame(exp5_before, columns = ['Source','Target'])\n",
        "exp5_before_attributes = exp5_before_df.merge(paper_author,how='left',on=('Source','Target'))\n",
        "exp5_before_attributes.to_csv('exp5_before_paper_author_attributes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eLj3czcBIrnY"
      },
      "outputs": [],
      "source": [
        "# Step 3: projection of BEFORE NI network\n",
        "# Before - ALL (not limited to NI-NI edges)\n",
        "exp5_before_net = nx.from_pandas_edgelist(exp5_before_attributes, source='Source', target='Target', edge_attr=True, create_using=nx.Graph())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lbHxZkN_IrnZ"
      },
      "outputs": [],
      "source": [
        "source_nodelist_before = exp5_before_attributes.Source ## papers only\n",
        "target_nodelist_before = exp5_before_attributes.Target  ## for authors only\n",
        "## unique list of attributes for source and target\n",
        "from collections import OrderedDict\n",
        "source_nodelist_before_sorted = sorted(list(OrderedDict.fromkeys(source_nodelist_before)))\n",
        "target_nodelist_before_sorted = sorted(list(OrderedDict.fromkeys(target_nodelist_before)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RmUtOI1IrnZ"
      },
      "outputs": [],
      "source": [
        "len(source_nodelist_before_sorted) ## 1861 initially, but down to 1758 unique papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lz8BK821d4u2"
      },
      "outputs": [],
      "source": [
        "subset_exp5_before = paper_author[paper_author[\"Source\"].isin(source_nodelist_before)]\n",
        "subset_exp5_before.shape[0] ## total paper-author edges based on 1769 unique papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGy041WRtzl5"
      },
      "outputs": [],
      "source": [
        "subset_exp5_before.to_csv('exp5_before_NI_all_edges.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T67RSBjtzl6"
      },
      "outputs": [],
      "source": [
        "exp5_before_all_net = nx.from_pandas_edgelist(subset_exp5_before, source='Source', target='Target', edge_attr=True, create_using=nx.Graph())\n",
        "target_before_full_sorted = sorted(list(OrderedDict.fromkeys(subset_exp5_before['Target'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V0xjOCgtzl6"
      },
      "outputs": [],
      "source": [
        "before_projection_all = bipartite.weighted_projected_graph(exp5_before_all_net, target_before_full_sorted, ratio=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmhBZoCh_Eko"
      },
      "outputs": [],
      "source": [
        "## PAPER projection\n",
        "before_projection_paper_paper = bipartite.weighted_projected_graph(exp5_before_all_net, source_nodelist_before_sorted, ratio=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4opALWbPtzl7"
      },
      "outputs": [],
      "source": [
        "number_of_nodes(before_projection_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qDpSl3Mtzl9"
      },
      "outputs": [],
      "source": [
        "number_of_edges(before_projection_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5DnWIoktzl-"
      },
      "outputs": [],
      "source": [
        "nx.write_weighted_edgelist(before_projection_all, \"exp5_before_projection.csv\", delimiter=',')\n",
        "paper_edgelist = pd.read_csv('exp5_before_projection.csv',names=[\"Source\",\"Target\",\"Weight\"])\n",
        "paper_edgelist.to_csv('exp5_before_projection.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_X48-sktzl-"
      },
      "outputs": [],
      "source": [
        "sorted(list(before_projection_all.degree(list(target_nodelist_before_sorted))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qeViao4zedBb"
      },
      "outputs": [],
      "source": [
        "len(before_projection_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUgsuEN3tzl-"
      },
      "outputs": [],
      "source": [
        "## NI-NI ties only\n",
        "before_NI_only = before_projection_all.subgraph(NI_only_nodelist['Target'])\n",
        "sorted(list(before_NI_only.degree(list(target_nodelist_before_sorted))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YoTqE1dtzl_"
      },
      "outputs": [],
      "source": [
        "number_of_nodes(before_NI_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yBHOF5GZtzl_"
      },
      "outputs": [],
      "source": [
        "number_of_edges(before_NI_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wFTxWvztzl_"
      },
      "outputs": [],
      "source": [
        "nx.write_weighted_edgelist(before_NI_only, \"exp5_before_NI_only_projection.csv\", delimiter=',')\n",
        "paper_edgelist = pd.read_csv('exp5_before_NI_only_projection.csv',names=[\"Source\",\"Target\",\"Weight\"])\n",
        "paper_edgelist.to_csv('exp5_before_NI_only_projection.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLew4gyEtzmA"
      },
      "outputs": [],
      "source": [
        "before_all_list = pd.DataFrame(sorted(list(before_projection_all.degree(list(target_nodelist_before_sorted)))), columns = ['node','count'])\n",
        "\n",
        "## NI-NI ties only\n",
        "before_NI_only = before_projection_all.subgraph(NI_only_nodelist['Target'])\n",
        "before_NI_list = pd.DataFrame(sorted(list(before_NI_only.degree(list(target_nodelist_before_sorted)))), columns = ['node','count'])\n",
        "\n",
        "exp5_before_egonets = before_all_list.merge(before_NI_list, how='inner', on='node')\n",
        "exp5_before_egonets.to_csv(\"exp5_before_egonets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VI5L_JtSRudu"
      },
      "outputs": [],
      "source": [
        "#paper-centric: get number of authors for each paper\n",
        "source_before_full_sorted = sorted(list(OrderedDict.fromkeys(subset_exp5_before['Source'])))\n",
        "degreeView = exp5_before_all_net.degree(source_before_full_sorted)\n",
        "degreeView = pd.DataFrame(degreeView, columns=['paper_name','num_coauthors'])\n",
        "degreeView.to_csv('exp5_before_paper_counts.csv',header=['paper_name','num_coauthors'])\n",
        "#degree_counts = Counter(dict(degreeView))\n",
        "\n",
        "print(\"\\n----------- Mean -----------\\n\")\n",
        "print(degreeView['num_coauthors'].mean())\n",
        "print(\"\\n----------- Median -----------\\n\")\n",
        "print(degreeView['num_coauthors'].median())\n",
        "print(\"\\n-----------  Mode -----------\\n\")\n",
        "print(degreeView['num_coauthors'].mode())\n",
        "print(\"\\n-----------  Stats -----------\\n\")\n",
        "print(degreeView['num_coauthors'].describe())\n",
        "print(\"\\n-----------  Skew -----------\\n\")\n",
        "print(degreeView['num_coauthors'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZjT6Nn7Q3Rw"
      },
      "outputs": [],
      "source": [
        "# paper-centric: get number of authors for each paper\n",
        "source_before_full_sorted = sorted(list(OrderedDict.fromkeys(subset_exp5_before['Source'])))\n",
        "degreeView = exp5_before_all_net.degree(source_before_full_sorted)\n",
        "degreeView = pd.DataFrame(degreeView, columns=['paper_name','num_coauthors'])\n",
        "degreeView.to_csv('exp5_before_paper_counts.csv',header=['paper_name','num_coauthors'])\n",
        "#degree_counts = Counter(dict(degreeView))\n",
        "\n",
        "print(\"\\n----------- Mean -----------\\n\")\n",
        "print(degreeView['num_coauthors'].mean())\n",
        "print(\"\\n----------- Median -----------\\n\")\n",
        "print(degreeView['num_coauthors'].median())\n",
        "print(\"\\n-----------  Mode -----------\\n\")\n",
        "print(degreeView['num_coauthors'].mode())\n",
        "print(\"\\n-----------  Stats -----------\\n\")\n",
        "print(degreeView['num_coauthors'].describe())\n",
        "print(\"\\n-----------  Skew -----------\\n\")\n",
        "print(degreeView['num_coauthors'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NPRUJwq-PdCx"
      },
      "outputs": [],
      "source": [
        "degreeView.to_csv(\"before_unique papers & authors counts.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USHYGABpRudu"
      },
      "outputs": [],
      "source": [
        "## ego-centric: for each ego, print # publications before vs after\n",
        "ego_pub_all = pd.DataFrame(exp5_before_all_net.degree(target_nodelist_before_sorted), columns=['author_name','num_papers_all'])\n",
        "ego_pub_coauthors_all = pd.DataFrame(before_projection_all.degree(target_nodelist_before_sorted), columns = ['author_name','co_authors_all'])\n",
        "ego_pub_coauthors_NIonly = pd.DataFrame(before_NI_only.degree(target_nodelist_before_sorted), columns = ['author_name','co_authors_NIonly'])\n",
        "\n",
        "ego_pub_final = pd.DataFrame()\n",
        "ego_pub_final['author_name'] = ego_pub_all['author_name']\n",
        "ego_pub_final['num_papers'] = ego_pub_all['num_papers_all']\n",
        "ego_pub_final['num_coauthors_all'] = ego_pub_coauthors_all['co_authors_all']\n",
        "ego_pub_final['num_coauthors_NIonly'] = ego_pub_coauthors_NIonly['co_authors_NIonly']\n",
        "ego_pub_final.to_csv('exp5_before_author_counts.csv',header=['author_name', 'num_papers', 'num_coauthors_all', 'num_coauthors_NIonly'])\n",
        "\n",
        "print(\"\\n----------- Mean -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].mean())\n",
        "print(\"\\n----------- Median -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].median())\n",
        "print(\"\\n-----------  Mode -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].mode())\n",
        "print(\"\\n-----------  Stats -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].describe())\n",
        "print(\"\\n-----------  Skew -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].skew())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkkybgjXM0kL"
      },
      "outputs": [],
      "source": [
        "ego_pub_all.to_csv(\"before_num_papers.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yjI5wbiBEPLM"
      },
      "outputs": [],
      "source": [
        "ego_pub_final['num_papers'].sum() ## not unique though, unique is above in count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxRDPM8UtzmB"
      },
      "outputs": [],
      "source": [
        " ## #Step 4: preparation of AFTER-NI: subset paper-author edges with appropriate pub range & gap size before-after\n",
        "exp5_after = [(u,v) for u,v,e in exp5_subset_net.edges(data=True) if e['Paper_Year'] - e['Author_NI_StartDate'] <= 4 and e['Paper_Year'] - e['Author_NI_StartDate'] >= 1 and e['Author_NI_StartDate'] != 0 and e['Paper_Year'] != 0 and e['Author_NI_StartDate'] != 9999 and e['Author_YearDegree'] != 9999]\n",
        "exp5_after_df = pd.DataFrame(exp5_after, columns = ['Source','Target'])\n",
        "exp5_after_attributes = exp5_after_df.merge(paper_author,how='left',on=('Source','Target'))\n",
        "exp5_after_attributes.to_csv('exp5_after_paper_author_attributes.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8HlouxKtzmC"
      },
      "outputs": [],
      "source": [
        "# Step 5: projection of AFTER NI network\n",
        "# AFTER - ALL (not limited to NI-NI edges)\n",
        "exp5_after_net = nx.from_pandas_edgelist(exp5_before_attributes, source='Source', target='Target', edge_attr=True, create_using=nx.Graph())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoGUOldntzmC"
      },
      "outputs": [],
      "source": [
        "source_nodelist_after = exp5_after_attributes.Source ## papers only\n",
        "target_nodelist_after = exp5_after_attributes.Target  ## for authors only\n",
        "## unique list of attributes for source and target\n",
        "from collections import OrderedDict\n",
        "source_nodelist_after_sorted = sorted(list(OrderedDict.fromkeys(source_nodelist_after)))\n",
        "target_nodelist_after_sorted = sorted(list(OrderedDict.fromkeys(target_nodelist_after)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zX2kGHi2tzmD"
      },
      "outputs": [],
      "source": [
        "len(source_nodelist_after_sorted) ## unique papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2yaDGiXtzmD"
      },
      "outputs": [],
      "source": [
        "subset_exp5_after = paper_author[paper_author[\"Source\"].isin(source_nodelist_after)]\n",
        "subset_exp5_after = subset_exp5_after[subset_exp5_after['Author_Department1'] != '9999']   ### added July 19\n",
        "subset_exp5_after.shape[0] ## total paper-author edges based on unique papers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6J-eR9rtzmE"
      },
      "outputs": [],
      "source": [
        "subset_exp5_after.to_csv('exp5_after_NI_all_edges.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnEzaPPPtzmE"
      },
      "outputs": [],
      "source": [
        "exp5_after_all_net = nx.from_pandas_edgelist(subset_exp5_after, source='Source', target='Target', edge_attr=True, create_using=nx.Graph())\n",
        "target_after_full_sorted = sorted(list(OrderedDict.fromkeys(subset_exp5_after['Target'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlyg7wQitzmE"
      },
      "outputs": [],
      "source": [
        "after_projection_all = bipartite.weighted_projected_graph(exp5_after_all_net, target_after_full_sorted, ratio=False)\n",
        "#list(before_projection_all.edges(data=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g2I_k5tp_bCh"
      },
      "outputs": [],
      "source": [
        "##Paper Projection\n",
        "after_projection_paper_paper = bipartite.weighted_projected_graph(exp5_after_all_net, source_nodelist_after_sorted, ratio=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxYDQzegtzmF"
      },
      "outputs": [],
      "source": [
        "number_of_nodes(after_projection_all) ## how many authors (all, not just NI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUfl3oLptzmF"
      },
      "outputs": [],
      "source": [
        "number_of_edges(after_projection_all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vM8SOPFtzmF"
      },
      "outputs": [],
      "source": [
        "nx.write_weighted_edgelist(before_projection_all, \"exp5_after_projection.csv\", delimiter=',')\n",
        "paper_edgelist = pd.read_csv('exp5_after_projection.csv',names=[\"Source\",\"Target\",\"Weight\"])\n",
        "paper_edgelist.to_csv('exp5_after_projection.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sp2LnNGtzmF"
      },
      "outputs": [],
      "source": [
        "sorted(list(after_projection_all.degree(list(target_nodelist_after_sorted))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcwEQqoFe1H3"
      },
      "outputs": [],
      "source": [
        "after_proj_list = list(after_projection_all)\n",
        "after_proj_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dspnx4wytzmG"
      },
      "outputs": [],
      "source": [
        "## NI-NI ties only\n",
        "after_NI_only = after_projection_all.subgraph(NI_only_nodelist['Target'])\n",
        "sorted(list(after_NI_only.degree(list(target_nodelist_after_sorted))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQaZyRU7tzmH"
      },
      "outputs": [],
      "source": [
        "number_of_nodes(after_NI_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vxOX1fv1tzmH"
      },
      "outputs": [],
      "source": [
        "number_of_edges(after_NI_only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kULaPcuatzmH"
      },
      "outputs": [],
      "source": [
        "nx.write_weighted_edgelist(after_NI_only, \"exp5_after_NI_only_projection.csv\", delimiter=',')\n",
        "paper_edgelist = pd.read_csv('exp5_after_NI_only_projection.csv',names=[\"Source\",\"Target\",\"Weight\"])\n",
        "paper_edgelist.to_csv('exp5_after_NI_only_projection.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q4ZzgT51tzmI"
      },
      "outputs": [],
      "source": [
        "after_all_list = pd.DataFrame(sorted(list(after_projection_all.degree(list(target_nodelist_after_sorted)))), columns = ['node','count'])\n",
        "\n",
        "## NI-NI ties only\n",
        "after_NI_only = after_projection_all.subgraph(NI_only_nodelist['Target'])\n",
        "after_NI_list = pd.DataFrame(sorted(list(after_NI_only.degree(list(target_nodelist_after_sorted)))), columns = ['node','count'])\n",
        "\n",
        "exp5_after_egonets = after_all_list.merge(after_NI_list, how='inner', on='node')\n",
        "exp5_after_egonets.to_csv(\"exp5_after_egonets.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DW-JPvZbSK6H"
      },
      "outputs": [],
      "source": [
        "## ego-centric: for each ego, print # publications before vs after\n",
        "ego_pub_all = pd.DataFrame(exp5_after_all_net.degree(target_nodelist_after_sorted), columns=['author_name','num_papers_all'])\n",
        "ego_pub_coauthors_all = pd.DataFrame(after_projection_all.degree(target_nodelist_after_sorted), columns = ['author_name','co_authors_all'])\n",
        "ego_pub_coauthors_NIonly = pd.DataFrame(after_NI_only.degree(target_nodelist_after_sorted), columns = ['author_name','co_authors_NIonly'])\n",
        "\n",
        "ego_pub_final = pd.DataFrame()\n",
        "ego_pub_final['author_name'] = ego_pub_all['author_name']\n",
        "ego_pub_final['num_papers'] = ego_pub_all['num_papers_all']\n",
        "ego_pub_final['num_coauthors_all'] = ego_pub_coauthors_all['co_authors_all']\n",
        "ego_pub_final['num_coauthors_NIonly'] = ego_pub_coauthors_NIonly['co_authors_NIonly']\n",
        "ego_pub_final.to_csv('exp5_after_author_counts.csv',header=['author_name', 'num_papers', 'num_coauthors_all', 'num_coauthors_NIonly'])\n",
        "\n",
        "print(\"\\n----------- Mean -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].mean())\n",
        "print(\"\\n----------- Median -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].median())\n",
        "print(\"\\n-----------  Mode -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].mode())\n",
        "print(\"\\n-----------  Stats -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].describe())\n",
        "print(\"\\n-----------  Skew -----------\\n\")\n",
        "print(ego_pub_final[['num_papers','num_coauthors_all','num_coauthors_NIonly']].skew())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAhVc8OySK6D"
      },
      "outputs": [],
      "source": [
        "#paper-centric: get number of authors for each paper\n",
        "source_after_full_sorted = sorted(list(OrderedDict.fromkeys(subset_exp5_after['Source'])))\n",
        "degreeView = exp5_after_all_net.degree(source_after_full_sorted)\n",
        "degreeView = pd.DataFrame(degreeView, columns=['paper_name','num_coauthors'])\n",
        "degreeView.to_csv('exp5_after_paper_counts.csv',header=['paper_name','num_coauthors'])\n",
        "#degree_counts = Counter(dict(degreeView))\n",
        "\n",
        "print(\"\\n----------- Mean -----------\\n\")\n",
        "print(degreeView['num_coauthors'].mean())\n",
        "print(\"\\n----------- Median -----------\\n\")\n",
        "print(degreeView['num_coauthors'].median())\n",
        "print(\"\\n-----------  Mode -----------\\n\")\n",
        "print(degreeView['num_coauthors'].mode())\n",
        "print(\"\\n-----------  Stats -----------\\n\")\n",
        "print(degreeView['num_coauthors'].describe())\n",
        "print(\"\\n-----------  Skew -----------\\n\")\n",
        "print(degreeView['num_coauthors'].skew())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qNjknl3G9Iq"
      },
      "outputs": [],
      "source": [
        "ego_pub_final.to_csv(\"after_num_papers.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2VTiZWu5w9M"
      },
      "outputs": [],
      "source": [
        "degreeView.to_csv(\"after_unique papers & authors counts.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfDIuQl0-nXN"
      },
      "source": [
        "## EGO-LEGEL ANALYSES"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njreh3fCA2p0"
      },
      "source": [
        "## Before EGO analysis - same dept"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OlYUZoSs-d7N"
      },
      "outputs": [],
      "source": [
        "edge_df = nx.to_pandas_edgelist(before_projection_all)  ## test either before_NI_only or before_projection_all\n",
        "edge_df.rename({'source':'source','target':'target'},axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMuwuO-E7spH"
      },
      "outputs": [],
      "source": [
        "edge_df_attributes = edge_df.merge(nodelist,how='left',left_on=('source'), right_on='Target', suffixes=('','_source')).merge(nodelist, how='left',left_on=('target'), right_on='Target', suffixes=('_source','_target')) ## the suffixes changes only when there's overlapping columns\n",
        "edge_df_attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y875Idu2gM1v"
      },
      "outputs": [],
      "source": [
        "before_ego_nets = []\n",
        "\n",
        "for node in list(before_projection_all.nodes(data=True)):\n",
        "    if node[0] in target_nodelist_before_sorted:\n",
        "      ego = ego_graph(before_projection_all, node[0], radius=1, center=True, undirected=False, distance=None)\n",
        "      ego.remove_edges_from(list(nx.selfloop_edges(ego)))\n",
        "      before_ego_nets.append((node[0], ego))\n",
        "    print(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l7GeX_5SCsXZ"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.DataFrame()\n",
        "\n",
        "## network computation in this loop\n",
        "\n",
        "for ego in before_ego_nets:\n",
        "  print(ego[0],ego[1].nodes) ## printing out the alters of the egonet\n",
        "  #  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target==target)]\n",
        "  #  print(attr)\n",
        "  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target.isin(ego[1].nodes))].append(edge_df_attributes[(edge_df_attributes.target==ego[0]) & (edge_df_attributes.source.isin(ego[1].nodes))]) ## compare if source and target is in nodes\n",
        "  ego_list = list(ego[1].nodes)\n",
        "  ego_list.remove(ego[0])  ## remove selfloop , need to convert to list to not impact the network object\n",
        "  attr['ego_key'] = str((ego[0],ego_list))\n",
        "  combined_df = combined_df.append(attr)\n",
        "#  print(attr)\n",
        "  attr.to_csv(ego[0] + 'ego.csv')\n",
        "combined_df.to_csv('before_egonet.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSRNrZOkrp7W"
      },
      "outputs": [],
      "source": [
        "## Shannon diversity for Before-NI\n",
        "combined_df = pd.DataFrame()\n",
        "\n",
        "##network computation in this loop\n",
        "\n",
        "for ego in before_ego_nets:\n",
        "  print(ego[0],ego[1].nodes) ## printing out the alters of the egonet\n",
        "  #  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target==target)]\n",
        "  #  print(attr)\n",
        "  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target.isin(ego[1].nodes))].append(edge_df_attributes[(edge_df_attributes.target==ego[0]) & (edge_df_attributes.source.isin(ego[1].nodes))]) ## compare if source and target is in nodes\n",
        "  source_dept = attr[[\"source\", \"Author_Department1_source\"]].drop_duplicates().reset_index(drop=True)\n",
        "  target_dept = attr[[\"target\", \"Author_Department1_target\"]].drop_duplicates().reset_index(drop=True)\n",
        "  source_dept.columns = [\"node\",\"department\"]\n",
        "  target_dept.columns = [\"node\",\"department\"]\n",
        "  combined_dept = source_dept.append(target_dept)\n",
        " # print(combined_dept.groupby(\"department\").count())\n",
        "  ## Shannon diversity, richness, evenness\n",
        "  counts = combined_dept.groupby(\"department\").count()\n",
        "  richness = combined_dept.groupby(\"department\").nunique().reset_index(drop=True)\n",
        "  counts[\"alters_have_dept\"] = (counts[\"node\"].sum())\n",
        "  counts[\"total_n_alters\"] = len(ego[1].nodes) - 1\n",
        "  counts[\"alters_have_dept / all_alters\"] = (counts[\"node\"].sum()) / (len(ego[1].nodes) - 1)\n",
        "  counts[\"simpson_per_node\"] =  ((counts[\"node\"] * (counts[\"node\"] - 1))  / ((counts[\"node\"].sum()) * (counts[\"node\"].sum() - 1)))  ## code added July 20\n",
        "  counts[\"simpson_diversity\"] = 1 - counts[\"simpson_per_node\"].sum()  ## July 20 --- Gini-Simpson index  ( 1 - Simpson so that the higher the number, the higher the diversity)\n",
        "  counts[\"richness\"] = richness[\"node\"].count()\n",
        "  counts[\"shannon_per_node\"] = - ((counts[\"node\"] / counts[\"node\"].sum()) * (np.log(counts[\"node\"] / counts[\"node\"].sum())))   ## put shannon formula here\n",
        "  counts[\"shannon_diversity\"] = counts[\"shannon_per_node\"].sum()\n",
        "  counts[\"shannon_equitability\"] = (counts[\"shannon_per_node\"].sum()) / (np.log(richness[\"node\"].count())) # Divide Shannon’s diversity index by natural logarithm of species richness ln(S) to calculate the Pielou evenness\n",
        "  ##Simpson equitability\n",
        "  counts[\"simpson_equitability\"] = (counts[\"simpson_per_node\"].sum()) / (np.log(richness[\"node\"].count())) # Divide Simpson’s diversity index by natural logarithm of species richness ln(S) to calculate evenness\n",
        "  ego_list = list(ego[1].nodes)\n",
        "  ego_list.remove(ego[0])  ## remove selfloop , need to convert to list to not impact the network object\n",
        "  counts['ego_key'] = str((ego[0],ego_list))\n",
        "  combined_df = combined_df.append(counts)\n",
        "  #print(counts)\n",
        "  counts.to_csv(ego[0] + 'ego.csv')\n",
        "combined_df.to_csv('before_NI_simpson & shannon.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K9d_GeUlNGdR"
      },
      "outputs": [],
      "source": [
        "#analysis on same department\n",
        "\n",
        "combined_samedept_df = pd.DataFrame()\n",
        "\n",
        "for ego in before_ego_nets:\n",
        "  print(ego[0],ego[1].nodes) ## printing out the alters of the egonet\n",
        "  #for target in ego[1].nodes:\n",
        "  #  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target==target)]\n",
        "  #  print(attr)\n",
        "  ## make sure it is an undirected version of egonets to also include egos that end up being in the 'target' column\n",
        "  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target.isin(ego[1].nodes))].append(edge_df_attributes[(edge_df_attributes.target==ego[0]) & (edge_df_attributes.source.isin(ego[1].nodes))]) ## compare if source and target is in nodes\n",
        "  department_source = attr[attr['source']==ego[0]].reset_index()\n",
        "  attr_department = pd.DataFrame()\n",
        "\n",
        "  if len(department_source.index) > 0:\n",
        "      dept_value = department_source.loc[0,'Author_Department1_source']\n",
        "      attr2 = department_source[dept_value == department_source['Author_Department1_target']]\n",
        "      attr_department = attr_department.append(attr2).drop_duplicates()\n",
        "  department_target = attr[attr['target']==ego[0]].reset_index()\n",
        "\n",
        "  if len(department_target.index) > 0:\n",
        "      dept_value2 = department_target.loc[0,'Author_Department1_target']\n",
        "      attr3 = department_target[dept_value2 == department_target['Author_Department1_source']]\n",
        "      attr_department = attr_department.append(attr3).drop_duplicates()\n",
        "  print(attr_department)\n",
        "\n",
        "  ego_list = list(ego[1].nodes)\n",
        "  ego_list.remove(ego[0])  ## remove selfloop , need to convert to list to not impact the network object\n",
        "  attr_department['ego_key'] = str((ego[0],ego_list))\n",
        "  combined_samedept_df = combined_samedept_df.append(attr_department)\n",
        "#combined_samedept_df.to_csv('combined_dept_ego.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qh2s1aEbCX4"
      },
      "outputs": [],
      "source": [
        "counts = combined_samedept_df.groupby('ego_key').count()[[\"source\"]].reset_index() ## do apply to extract the source from ego_key\n",
        "combined_samedept_df['ego_size'] = counts['source']\n",
        "combined_samedept_df.to_csv('before_NI_egonets_samedept.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1s2vVErPNGhG"
      },
      "outputs": [],
      "source": [
        "dept_value, dept_value2"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After EGO analysis - same dept"
      ],
      "metadata": {
        "id": "V-aAYCTO-Az0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJ1IKFwBm_6H"
      },
      "outputs": [],
      "source": [
        "edge_df = nx.to_pandas_edgelist(after_projection_all)  ## check either after_NI_only or after_projection_all\n",
        "edge_df.rename({'source':'source','target':'target'},axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcu6GAhsm_6I"
      },
      "outputs": [],
      "source": [
        "edge_df_attributes = edge_df.merge(nodelist,how='left',left_on=('source'), right_on='Target', suffixes=('','_source')).merge(nodelist, how='left',left_on=('target'), right_on='Target', suffixes=('_source','_target')) ## the suffixes changes only when there's overlapping columns\n",
        "edge_df_attributes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KHcmQs8m_6K"
      },
      "outputs": [],
      "source": [
        "after_ego_nets = []\n",
        "\n",
        "for node in list(after_projection_all.nodes(data=True)):\n",
        "    if node[0] in target_nodelist_after_sorted:\n",
        "      ego = ego_graph(after_projection_all, node[0], radius=1, center=True, undirected=False, distance=None)\n",
        "      after_ego_nets.append((node[0], ego))\n",
        "    print(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix1M5iHEm_6L"
      },
      "outputs": [],
      "source": [
        "combined_after_df = pd.DataFrame()\n",
        "\n",
        "## network computation in this loop\n",
        "\n",
        "for ego in after_ego_nets:\n",
        "  print(ego[0],ego[1].nodes) ## printing out the alters of the egonet\n",
        "  #  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target==target)]\n",
        "  #  print(attr)\n",
        "  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target.isin(ego[1].nodes))].append(edge_df_attributes[(edge_df_attributes.target==ego[0]) & (edge_df_attributes.source.isin(ego[1].nodes))]) ## compare if source and target is in nodes\n",
        "  ego_list = list(ego[1].nodes)\n",
        "  ego_list.remove(ego[0])  ## remove selfloop , need to convert to list to not impact the network object\n",
        "  attr['ego_key'] = str((ego[0],ego_list))\n",
        "  combined_after_df = combined_after_df.append(attr)\n",
        "  print(attr)\n",
        "  attr.to_csv(ego[0] + 'ego.csv')\n",
        "combined_after_df.to_csv('after_egonet.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50ZXMSVjCAaK"
      },
      "outputs": [],
      "source": [
        "#analysis on same department\n",
        "\n",
        "after_combined_samedept_df = pd.DataFrame()\n",
        "\n",
        "for ego in after_ego_nets:\n",
        "  print(ego[0],ego[1].nodes) ## printing out the alters of the egonet\n",
        "  #for target in ego[1].nodes:\n",
        "  #  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target==target)]\n",
        "  #  print(attr)\n",
        "  ## make sure it is an undirected version of egonets to also include egos that end up being in the 'target' column\n",
        "  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target.isin(ego[1].nodes))].append(edge_df_attributes[(edge_df_attributes.target==ego[0]) & (edge_df_attributes.source.isin(ego[1].nodes))]) ## compare if source and target is in nodes\n",
        "  department_source = attr[attr['source']==ego[0]].reset_index()\n",
        "  attr_department = pd.DataFrame()\n",
        "\n",
        "  if len(department_source.index) > 0:\n",
        "      dept_value = department_source.loc[0,'Author_Department1_source']\n",
        "      attr2 = department_source[dept_value == department_source['Author_Department1_target']]\n",
        "      attr_department = attr_department.append(attr2).drop_duplicates()\n",
        "  department_target = attr[attr['target']==ego[0]].reset_index()\n",
        "\n",
        "  if len(department_target.index) > 0:\n",
        "      dept_value2 = department_target.loc[0,'Author_Department1_target']\n",
        "      attr3 = department_target[dept_value2 == department_target['Author_Department1_source']]\n",
        "      attr_department = attr_department.append(attr3).drop_duplicates()\n",
        "  print(attr_department)\n",
        "\n",
        "  ego_list = list(ego[1].nodes)\n",
        "  ego_list.remove(ego[0])  ## remove selfloop , need to convert to list to not impact the network object\n",
        "  attr_department['ego_key'] = str((ego[0],ego_list))\n",
        "  after_combined_samedept_df = after_combined_samedept_df.append(attr_department)\n",
        "#combined_samedept_df.to_csv('after_combined_dept_ego.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "48Y_lYP-CAaL"
      },
      "outputs": [],
      "source": [
        "counts = after_combined_samedept_df.groupby('ego_key').count()[[\"source\"]].reset_index() ## do apply to extract the source from ego_key\n",
        "after_combined_samedept_df['ego_size'] = counts['source']\n",
        "after_combined_samedept_df.to_csv('after_NI_egonets_samedept.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVjIgKc7D_GG"
      },
      "outputs": [],
      "source": [
        "## Shannon diversity for AFTER\n",
        "combined_after_df = pd.DataFrame()\n",
        "\n",
        "## do network computation in this loop\n",
        "\n",
        "for ego in after_ego_nets:\n",
        "  print(ego[0],ego[1].nodes) ## printing out the alters of the egonet\n",
        "  #  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target==target)]\n",
        "  #  print(attr)\n",
        "  attr = edge_df_attributes[(edge_df_attributes.source==ego[0]) & (edge_df_attributes.target.isin(ego[1].nodes))].append(edge_df_attributes[(edge_df_attributes.target==ego[0]) & (edge_df_attributes.source.isin(ego[1].nodes))]) ## compare if source and target is in nodes\n",
        "  source_dept = attr[[\"source\", \"Author_Department1_source\"]].drop_duplicates().reset_index(drop=True)\n",
        "  target_dept = attr[[\"target\", \"Author_Department1_target\"]].drop_duplicates().reset_index(drop=True)\n",
        "  source_dept.columns = [\"node\",\"department\"]\n",
        "  target_dept.columns = [\"node\",\"department\"]\n",
        "  combined_dept = source_dept.append(target_dept)\n",
        " # print(combined_dept.groupby(\"department\").count())\n",
        "  ## Shannon diversity, richness, evenness\n",
        "  counts = combined_dept.groupby(\"department\").count()\n",
        "  richness = combined_dept.groupby(\"department\").nunique().reset_index(drop=True)\n",
        "  counts[\"alters_have_dept\"] = (counts[\"node\"].sum())\n",
        "  counts[\"total_n_alters\"] = len(ego[1].nodes) - 1\n",
        "  counts[\"alters_have_dept / all_alters\"] = (counts[\"node\"].sum()) / (len(ego[1].nodes) - 1)\n",
        "  counts[\"simpson_per_node\"] =  ((counts[\"node\"] * (counts[\"node\"] - 1))  / ((counts[\"node\"].sum()) * (counts[\"node\"].sum() - 1)))  ## code added July 20\n",
        "  counts[\"simpson_diversity\"] = 1 - counts[\"simpson_per_node\"].sum()  ## July 20 --- Gini-Simpson index  ( 1 - Simpson so that the higher the number, the higher the diversity)\n",
        "  counts[\"richness\"] = richness[\"node\"].count()\n",
        "  counts[\"shannon_per_node\"] = - ((counts[\"node\"] / counts[\"node\"].sum()) * (np.log(counts[\"node\"] / counts[\"node\"].sum())))   ## put shannon formula here\n",
        "  counts[\"shannon_diversity\"] = counts[\"shannon_per_node\"].sum()\n",
        "  counts[\"shannon_equitability\"] = (counts[\"shannon_per_node\"].sum()) / (np.log(richness[\"node\"].count())) # Divide Shannon’s diversity index by natural logarithm of species richness ln(S) to calculate the Pielou evenness\n",
        "  ##Simpson equitability\n",
        "  counts[\"simpson_equitability\"] = (counts[\"simpson_per_node\"].sum()) / (np.log(richness[\"node\"].count())) # Divide Simpson’s diversity index by natural logarithm of species richness ln(S) to calculate evenness\n",
        "  ego_list = list(ego[1].nodes)\n",
        "  ego_list.remove(ego[0])  ## remove selfloop , need to convert to list to not impact the network object\n",
        "  counts['ego_key'] = str((ego[0],ego_list))\n",
        "  combined_after_df = combined_after_df.append(counts)\n",
        "  #print(counts)\n",
        "  counts.to_csv(ego[0] + 'ego.csv')\n",
        "combined_after_df.to_csv('after_NI_simpson & shannon.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}